{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnITccKqxtOq"
   },
   "source": [
    "# 0.Install Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16169,
     "status": "ok",
     "timestamp": 1735288569929,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "OzSN9gT8PvAW",
    "outputId": "66e98201-1434-42b9-9bed-6161832976c4"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.47.0\n",
    "!pip install datasets==3.2.0\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1735288570863,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "erkR7rjytKyU"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"Add hugingface token here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm5wZzWNltUt"
   },
   "source": [
    "Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2053,
     "status": "ok",
     "timestamp": 1735288572913,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "bLLE1nv7jQrn",
    "outputId": "efcdb1b9-2b75-4670-bbf4-b7f947580f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive to storage model \n",
    "drive.mount('/content/drive')\n",
    "cache_dir=\"/content/drive/MyDrive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EESH0_Zlx0aA"
   },
   "source": [
    "# 1.Load Llama model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XM9o-REvKNJB"
   },
   "source": [
    "**Download Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37275,
     "status": "ok",
     "timestamp": 1735288610186,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "Wm4xLIyes1oC",
    "outputId": "47c65206-2466-41b8-bdbc-1e1fbe733305"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAedz47yrj9I"
   },
   "source": [
    "# 2.Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1735288610514,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "biBXlJjHFhOR"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1735288611237,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "yUwZGwihEIkR"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "  {\n",
    "    \"question\": \"H√£y cho t√¥i bi·∫øt n·ªôi dung c·ªßa kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c\",\n",
    "    \"answer\": \"Kho√° h·ªçc bao g·ªìm c√°c ch·ªß ƒë·ªÅ v·ªÅ Generative AI, large language models, langchain, RAG, v√† fine-tuning m√¥ h√¨nh LLM.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Ai l√† ƒë·ªëi t∆∞·ª£ng ph√π h·ª£p tham gia kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c?\",\n",
    "    \"answer\": \"Kho√° h·ªçc ph√π h·ª£p cho c√°c l·∫≠p tr√¨nh vi√™n, nh√† nghi√™n c·ª©u AI, v√† nh·ªØng ng∆∞·ªùi quan t√¢m ƒë·∫øn c√¥ng ngh·ªá Generative AI.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c k√©o d√†i bao l√¢u?\",\n",
    "    \"answer\": \"Kho√° h·ªçc k√©o d√†i kho·∫£ng 9 gi·ªù, bao g·ªìm c·∫£ l√Ω thuy·∫øt v√† th·ª±c h√†nh.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ n·ªôi dung th·ª±c h√†nh kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, kho√° h·ªçc t·∫≠p trung nhi·ªÅu v√†o th·ª±c h√†nh, chi·∫øm h∆°n 70% th·ªùi l∆∞·ª£ng kh√≥a h·ªçc.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Trong kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ ƒë·ªÅ c·∫≠p ƒë·∫øn Langchain kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, Langchain l√† m·ªôt trong c√°c n·ªôi dung ch√≠nh c·ªßa kho√° h·ªçc, gi√∫p hi·ªÉu c√°ch x√¢y d·ª±ng ·ª©ng d·ª•ng AI.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ ph√π h·ª£p cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, kho√° h·ªçc ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ph√π h·ª£p cho c·∫£ ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu l·∫´n ng∆∞·ªùi c√≥ kinh nghi·ªám.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"H·ªçc ph√≠ c·ªßa kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c l√† bao nhi√™u?\",\n",
    "    \"answer\": \"H·ªçc ph√≠ c·ª• th·ªÉ s·∫Ω ƒë∆∞·ª£c th√¥ng b√°o tr√™n trang ch√≠nh th·ª©c, nh∆∞ng r·∫•t c·∫°nh tranh v√† h·ª£p l√Ω so v·ªõi gi√° tr·ªã mang l·∫°i.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ c·∫•p ch·ª©ng ch·ªâ kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, b·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c ch·ª©ng ch·ªâ ho√†n th√†nh kh√≥a h·ªçc sau khi ho√†n t·∫•t c√°c b√†i t·∫≠p v√† b√†i ki·ªÉm tra cu·ªëi kho√°.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ c·∫ßn ki·∫øn th·ª©c l·∫≠p tr√¨nh kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, kho√° h·ªçc y√™u c·∫ßu b·∫°n c√≥ ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ Python tr∆∞·ªõc khi tham gia.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ h·ªçc tr·ª±c tuy·∫øn kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, kho√° h·ªçc ƒë∆∞·ª£c t·ªï ch·ª©c ho√†n to√†n tr·ª±c tuy·∫øn ƒë·ªÉ ti·ªán l·ª£i cho h·ªçc vi√™n.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ ph√π h·ª£p ƒë·ªÉ ph√°t tri·ªÉn k·ªπ nƒÉng th·ª±c t·∫ø kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, kho√° h·ªçc t·∫≠p trung v√†o c√°c k·ªπ nƒÉng th·ª±c t·∫ø nh∆∞ fine-tuning m√¥ h√¨nh v√† t√≠ch h·ª£p Generative AI v√†o ·ª©ng d·ª•ng.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"T√¥i c·∫ßn chu·∫©n b·ªã g√¨ tr∆∞·ªõc khi tham gia kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c?\",\n",
    "    \"answer\": \"B·∫°n c·∫ßn chu·∫©n b·ªã ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ Python v√† kh·∫£ nƒÉng s·ª≠ d·ª•ng m√°y t√≠nh c√° nh√¢n ƒë·ªÉ th·ª±c h√†nh.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ √°p d·ª•ng trong ng√†nh c√¥ng nghi·ªáp kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, c√°c ki·∫øn th·ª©c t·ª´ kho√° h·ªçc c√≥ th·ªÉ √°p d·ª•ng trong nhi·ªÅu ng√†nh nh∆∞ t√†i ch√≠nh, y t·∫ø, v√† gi√°o d·ª•c.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ t√†i li·ªáu h·ªçc kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, t√†i li·ªáu h·ªçc v√† b√†i t·∫≠p th·ª±c h√†nh ƒë∆∞·ª£c cung c·∫•p ƒë·∫ßy ƒë·ªß trong kho√° h·ªçc.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ h·ªó tr·ª£ sau kho√° h·ªçc kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, h·ªçc vi√™n ƒë∆∞·ª£c h·ªó tr·ª£ gi·∫£i ƒë√°p th·∫Øc m·∫Øc v√† ti·∫øp c·∫≠n t√†i nguy√™n b·ªï sung sau khi ho√†n th√†nh kho√° h·ªçc.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Generative AI l√† g√¨, v√† n√≥ ƒë∆∞·ª£c gi·∫£ng d·∫°y nh∆∞ th·∫ø n√†o trong kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c?\",\n",
    "    \"answer\": \"Generative AI l√† c√¥ng ngh·ªá t·∫°o ra n·ªôi dung m·ªõi t·ª´ d·ªØ li·ªáu. Trong kho√° h·ªçc, b·∫°n s·∫Ω h·ªçc c√°ch x√¢y d·ª±ng v√† ·ª©ng d·ª•ng c√°c m√¥ h√¨nh Generative AI.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ ph√π h·ª£p v·ªõi nh√† nghi√™n c·ª©u kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, kho√° h·ªçc cung c·∫•p ki·∫øn th·ª©c chuy√™n s√¢u v√† c√°c c√¥ng c·ª• hi·ªán ƒë·∫°i, ph√π h·ª£p cho c√°c nh√† nghi√™n c·ª©u AI.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ b√†i t·∫≠p nh√≥m kh√¥ng?\",\n",
    "    \"answer\": \"Kh√¥ng, nh∆∞ng c√≥ nhi·ªÅu b√†i t·∫≠p c√° nh√¢n ƒë·ªÉ h·ªçc vi√™n th·ª±c h√†nh v√† c·ªßng c·ªë ki·∫øn th·ª©c.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c s·∫Ω gi√∫p t√¥i h·ªçc g√¨ v·ªÅ fine-tuning m√¥ h√¨nh?\",\n",
    "    \"answer\": \"B·∫°n s·∫Ω h·ªçc c√°ch ƒëi·ªÅu ch·ªânh m√¥ h√¨nh LLM ƒë·ªÉ ph√π h·ª£p v·ªõi c√°c t·∫≠p d·ªØ li·ªáu c·ª• th·ªÉ v√† t·ªëi ∆∞u ho√° hi·ªáu su·∫•t.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ bao g·ªìm ph·∫ßn h∆∞·ªõng d·∫´n v·ªÅ RAG kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, ph·∫ßn RAG gi√∫p h·ªçc vi√™n hi·ªÉu c√°ch k·∫øt h·ª£p Generative AI v·ªõi d·ªØ li·ªáu b√™n ngo√†i ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"T·∫°i sao n√™n h·ªçc kho√° Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c?\",\n",
    "    \"answer\": \"Kho√° h·ªçc kh√¥ng ch·ªâ cung c·∫•p ki·∫øn th·ª©c l√Ω thuy·∫øt m√† c√≤n h∆∞·ªõng d·∫´n th·ª±c h√†nh √°p d·ª•ng tr·ª±c ti·∫øp trong c√¥ng vi·ªác.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ ƒëi·ªÉm g√¨ ƒë·∫∑c bi·ªát so v·ªõi c√°c kho√° h·ªçc kh√°c?\",\n",
    "    \"answer\": \"Kh√≥a h·ªçc k·∫øt h·ª£p gi·ªØa l√Ω thuy·∫øt, th·ª±c h√†nh chuy√™n s√¢u v√† kinh nghi·ªám th·ª±c t·∫ø c·ªßa gi·∫£ng vi√™n Nguy·ªÖn M·∫°nh L·ª±c.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Ng√¥n ng·ªØ s·ª≠ d·ª•ng trong kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c l√† g√¨?\",\n",
    "    \"answer\": \"Kho√° h·ªçc s·ª≠ d·ª•ng ti·∫øng Vi·ªát l√†m ng√¥n ng·ªØ ch√≠nh, nh∆∞ng c√≥ th·ªÉ c√≥ t√†i li·ªáu tham kh·∫£o b·∫±ng ti·∫øng Anh.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"H·ªçc vi√™n c·∫ßn m√°y t√≠nh c·∫•u h√¨nh ra sao ƒë·ªÉ tham gia kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c?\",\n",
    "    \"answer\": \"M√°y t√≠nh c·∫ßn c·∫•u h√¨nh trung b√¨nh v·ªõi √≠t nh·∫•t 8GB RAM v√† CPU t·ªët ƒë·ªÉ ch·∫°y c√°c b√†i t·∫≠p AI.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ ƒë√†o t·∫°o v·ªÅ LLM kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, b·∫°n s·∫Ω h·ªçc c√°ch l√†m vi·ªác v·ªõi Large Language Models v√† c√°c ·ª©ng d·ª•ng th·ª±c ti·ªÖn c·ªßa ch√∫ng.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ ph√π h·ª£p v·ªõi ng∆∞·ªùi l√†m kinh doanh kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, kh√≥a h·ªçc gi√∫p ng∆∞·ªùi l√†m kinh doanh hi·ªÉu v√† √°p d·ª•ng Generative AI ƒë·ªÉ t·ªëi ∆∞u h√≥a quy tr√¨nh v√† n√¢ng cao hi·ªáu su·∫•t.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Ng∆∞·ªùi h·ªçc s·∫Ω ƒë·∫°t ƒë∆∞·ª£c g√¨ sau khi ho√†n th√†nh kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c?\",\n",
    "    \"answer\": \"B·∫°n s·∫Ω n·∫Øm v·ªØng ki·∫øn th·ª©c v·ªÅ Generative AI v√† c√≥ th·ªÉ t·ª± tin √°p d·ª•ng v√†o c√°c d·ª± √°n th·ª±c t·∫ø.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kh√≥a h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ h·ªó tr·ª£ x√¢y d·ª±ng chatbot kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, b·∫°n s·∫Ω ƒë∆∞·ª£c h·ªçc c√°ch t√≠ch h·ª£p Generative AI v√†o x√¢y d·ª±ng chatbot th√¥ng minh.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c c√≥ s·ª≠ d·ª•ng c√°c m√¥ h√¨nh m√£ ngu·ªìn m·ªü kh√¥ng?\",\n",
    "    \"answer\": \"C√≥, b·∫°n s·∫Ω ƒë∆∞·ª£c h·ªçc c√°ch s·ª≠ d·ª•ng v√† t√πy ch·ªânh c√°c m√¥ h√¨nh m√£ ngu·ªìn m·ªü nh∆∞ GPT v√† BERT.\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1735288611237,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "tv2VzQmQEIq9"
   },
   "outputs": [],
   "source": [
    "questions = [item[\"question\"] for item in dataset]\n",
    "answers = [item[\"answer\"] for item in dataset]\n",
    "\n",
    "# Create a dictionary of lists\n",
    "dict_data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1735288611237,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "8qH6ZzYMEIyO",
    "outputId": "1683e1a5-4b9e-4c46-dd88-1709158ecd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 29\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_dict(dict_data)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "daa413a83f534cdb8ce5d41e0fe2e5bc",
      "a0ba698e241d47d49c34d8ca909dfa80",
      "86577e0527b94cdfaac29fa8fc970861",
      "8177beb385b9496398a016f13b631312",
      "3175a57a7b0a487ca3aac13c252cfe4c",
      "cc2b555126364edf827dc87d07467fdf",
      "1e6012e508c14279a2631935059b83b7",
      "11c67769394c4f0b92ebb1f98557c1df",
      "aa1b8455e22f415e87628ab3deedf433",
      "e15b43353d7f404b810c41d45a8b1fb0",
      "193f6d8427e442798db6fa2557198944"
     ]
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1735288611237,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "YqGO0aZHmxse",
    "outputId": "d028ee8c-dd35-4668-e8be-ef9e77e20df9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa413a83f534cdb8ce5d41e0fe2e5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_instruction_prompt(example):\n",
    "    instruction = example['question']\n",
    "    response = example['answer']\n",
    "    example['text'] = f\"<s>Question: {instruction}\\nAnswer: {response}</s>\"\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(create_instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1735288611238,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "viCyyKzQmzvY",
    "outputId": "61f54d19-3249-4ebe-9439-f4a3bb779649"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'H√£y cho t√¥i bi·∫øt n·ªôi dung c·ªßa kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c',\n",
       " 'answer': 'Kho√° h·ªçc bao g·ªìm c√°c ch·ªß ƒë·ªÅ v·ªÅ Generative AI, large language models, langchain, RAG, v√† fine-tuning m√¥ h√¨nh LLM.',\n",
       " 'text': '<s>Question: H√£y cho t√¥i bi·∫øt n·ªôi dung c·ªßa kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c\\nAnswer: Kho√° h·ªçc bao g·ªìm c√°c ch·ªß ƒë·ªÅ v·ªÅ Generative AI, large language models, langchain, RAG, v√† fine-tuning m√¥ h√¨nh LLM.</s>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Sz0w2M7n5M_"
   },
   "source": [
    "# 3.Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1735288611238,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "1et1DjAdol_e"
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fb7686045ddd4cf3b0f4b1ad21e8d1e4",
      "b14532471714421fa8eb555d9787dc4b",
      "ff36851e25b64c68b43914996f6dd5f9",
      "4842ede275d4400588c02d3b271bf213",
      "26099e8bbef14b6b97513c776d7ee88e",
      "fcacac691ef146db86f2029a812a2f2c",
      "45b2a8d17362417c81707ba66ecda4db",
      "4a79e2c4516b4e4eb551b41df0c42372",
      "9666aa483a8c404ab6dcc0c57ad085b8",
      "86fa6ff3a6d947eb89cfa7c06de78df6",
      "981d37e22e374e4fbbc70142ef562255"
     ]
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1735288611620,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "sa_q3QF4n14w",
    "outputId": "bb6469c4-6c1e-4b44-c1ec-a41ed1e239e4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7686045ddd4cf3b0f4b1ad21e8d1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",  # 512\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Apply the tokenization function\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"question\", \"answer\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1735288611621,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "7_-oOaTNor7w",
    "outputId": "f8f87aa3-7242-4ac3-b5da-63b521fdf771"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[0].get(\"input_ids\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGlp_tNnF_sE"
   },
   "source": [
    "# 4.Setup LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1735288611621,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "Ca_3XqSZF-mn"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1735288611939,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "c3Ojz1iDF-He",
    "outputId": "be1bcf3b-fcf2-4bc8-e8b4-63c7ce6ea39f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                # Rank of the LoRA updates\n",
    "    lora_alpha=32,      # Scaling factor\n",
    "    lora_dropout=0.1,   # Dropout to prevent overfitting\n",
    "    bias=\"none\",        # No bias adaptation\n",
    "    task_type=\"CAUSAL_LM\"  # Task type\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Verify that only LoRA layers are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1735288611939,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "WzWT9OQwtuMR"
   },
   "outputs": [],
   "source": [
    "# Use the default data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # MLM (Masked Language Modeling) is False for causal LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1735288611939,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "NgW09mVaEJmW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BbMcrN-p_xB"
   },
   "source": [
    "# 5.Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1735288612396,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "rx6tqEqF9ygz"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1735288612814,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "OI7YCgw0qG-E",
    "outputId": "a9923850-d5fd-495a-f45a-6ef74361a999"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/01_Fine_TuningLLM/models/lora_finetuned_llama\",  # Directory to save checkpoints\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=100,            # Number of epochs\n",
    "    per_device_train_batch_size=4, # Adjust based on your GPU memory\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients to reduce memory usage\n",
    "    evaluation_strategy=\"no\",      # No evaluation for small datasets\n",
    "    save_strategy=\"epoch\",         # Save model at the end of each epoch\n",
    "    learning_rate=3e-4,            # Learning rate for fine-tuning\n",
    "    fp16=True,                     # Enable mixed precision for faster training\n",
    "    logging_dir=\"./logs\",          # Directory for logs\n",
    "    logging_steps=50,              # Log every 50 steps\n",
    "    save_total_limit=2,            # Keep only 2 checkpoints\n",
    "    report_to=\"none\"               # Disable reporting (e.g., WandB)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2501,
     "status": "ok",
     "timestamp": 1735288615310,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "nfloHzTVqJFW",
    "outputId": "ffa6e7e1-af85-4423-b428-514d2009fd52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-e67d5074adcf>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                           # LoRA-wrapped model\n",
    "    args=training_args,                    # Training arguments\n",
    "    train_dataset=tokenized_dataset,       # Tokenized dataset\n",
    "    data_collator=data_collator,           # Data collator\n",
    "    tokenizer=tokenizer                    # Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 637877,
     "status": "ok",
     "timestamp": 1735289253183,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "yOBs6khyqTkH",
    "outputId": "cc4180c3-f7fc-423b-e6c2-63cb140bd93a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 10:32, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.213900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=2.2345990753173828, metrics={'train_runtime': 637.2308, 'train_samples_per_second': 4.551, 'train_steps_per_second': 0.314, 'total_flos': 8677154095104000.0, 'train_loss': 2.2345990753173828, 'epoch': 100.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1735289253527,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "TFaavJRusbKY"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/drive/MyDrive/01_Fine_TuningLLM/models/lora_finetuned_llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735289253528,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "PPwbHjx0sasb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSJWrOy5seNT"
   },
   "source": [
    "# 6.Load Fine-Tunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1735289253528,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "L3LSGm4GsaJu"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 9687,
     "status": "ok",
     "timestamp": 1735289263212,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "4WOMs-a7som3"
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", cache_dir=cache_dir)\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/01_Fine_TuningLLM/models/lora_finetuned_llama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2054,
     "status": "ok",
     "timestamp": 1735289265250,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "RO7Iu17AE0qF",
    "outputId": "b46027af-3f1e-4064-bf53-276f72271711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1855,
     "status": "ok",
     "timestamp": 1735289267097,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "8MALtVFvquKb",
    "outputId": "bb55dda2-42cc-4051-c8c8-c717e34f6c20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Question: H√£y cho t√¥i bi·∫øt n·ªôi dung c·ªßa kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c \n",
      "Answer: Kho√° h·ªçc bao g·ªìm c√°c ch·ªß ƒë·ªÅ v·ªÅ Generative AI, large language models, langchain, RAG, v√† fine-tuning m√¥ h√¨nh LLM.</s>\n"
     ]
    }
   ],
   "source": [
    "# Input prompt\n",
    "prompt = \"<s>Question: H√£y cho t√¥i bi·∫øt n·ªôi dung c·ªßa kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c \\nAnswer:\"\n",
    "\n",
    "# Generate text\n",
    "response = generator(prompt, max_length=200, num_return_sequences=1, do_sample=True)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1651,
     "status": "ok",
     "timestamp": 1735289268743,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "qBsEJLBKwKc4",
    "outputId": "f8c64d0f-42b9-4b3c-cecc-331cd1265181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Question: Ai l√† ƒë·ªëi t∆∞·ª£ng ph√π h·ª£p tham gia kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c? \n",
      "Answer: Kho√° h·ªçc ph√π h·ª£p cho c√°c l·∫≠p tr√¨nh vi√™n, nh√† nghi√™n c·ª©u AI, v√† nh·ªØng ng∆∞·ªùi quan t√¢m ƒë·∫øn c√¥ng ngh·ªá Generative AI.</s>\n"
     ]
    }
   ],
   "source": [
    "# Input prompt\n",
    "prompt = \"<s>Question: Ai l√† ƒë·ªëi t∆∞·ª£ng ph√π h·ª£p tham gia kho√° h·ªçc Generative AI c·ªßa Nguy·ªÖn M·∫°nh L·ª±c? \\nAnswer:\"\n",
    "\n",
    "# Generate text\n",
    "response = generator(prompt, max_length=200, num_return_sequences=1, do_sample=True)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1735289268744,
     "user": {
      "displayName": "Luc Nguyen",
      "userId": "11685761023964702149"
     },
     "user_tz": -540
    },
    "id": "T-MdK7DvQQch"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11c67769394c4f0b92ebb1f98557c1df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "193f6d8427e442798db6fa2557198944": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e6012e508c14279a2631935059b83b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26099e8bbef14b6b97513c776d7ee88e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3175a57a7b0a487ca3aac13c252cfe4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45b2a8d17362417c81707ba66ecda4db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4842ede275d4400588c02d3b271bf213": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86fa6ff3a6d947eb89cfa7c06de78df6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_981d37e22e374e4fbbc70142ef562255",
      "value": "‚Äá29/29‚Äá[00:00&lt;00:00,‚Äá369.90‚Äáexamples/s]"
     }
    },
    "4a79e2c4516b4e4eb551b41df0c42372": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8177beb385b9496398a016f13b631312": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e15b43353d7f404b810c41d45a8b1fb0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_193f6d8427e442798db6fa2557198944",
      "value": "‚Äá29/29‚Äá[00:00&lt;00:00,‚Äá567.85‚Äáexamples/s]"
     }
    },
    "86577e0527b94cdfaac29fa8fc970861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11c67769394c4f0b92ebb1f98557c1df",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aa1b8455e22f415e87628ab3deedf433",
      "value": 29
     }
    },
    "86fa6ff3a6d947eb89cfa7c06de78df6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9666aa483a8c404ab6dcc0c57ad085b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "981d37e22e374e4fbbc70142ef562255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0ba698e241d47d49c34d8ca909dfa80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc2b555126364edf827dc87d07467fdf",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1e6012e508c14279a2631935059b83b7",
      "value": "Map:‚Äá100%"
     }
    },
    "aa1b8455e22f415e87628ab3deedf433": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b14532471714421fa8eb555d9787dc4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcacac691ef146db86f2029a812a2f2c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_45b2a8d17362417c81707ba66ecda4db",
      "value": "Map:‚Äá100%"
     }
    },
    "cc2b555126364edf827dc87d07467fdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daa413a83f534cdb8ce5d41e0fe2e5bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a0ba698e241d47d49c34d8ca909dfa80",
       "IPY_MODEL_86577e0527b94cdfaac29fa8fc970861",
       "IPY_MODEL_8177beb385b9496398a016f13b631312"
      ],
      "layout": "IPY_MODEL_3175a57a7b0a487ca3aac13c252cfe4c"
     }
    },
    "e15b43353d7f404b810c41d45a8b1fb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb7686045ddd4cf3b0f4b1ad21e8d1e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b14532471714421fa8eb555d9787dc4b",
       "IPY_MODEL_ff36851e25b64c68b43914996f6dd5f9",
       "IPY_MODEL_4842ede275d4400588c02d3b271bf213"
      ],
      "layout": "IPY_MODEL_26099e8bbef14b6b97513c776d7ee88e"
     }
    },
    "fcacac691ef146db86f2029a812a2f2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff36851e25b64c68b43914996f6dd5f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a79e2c4516b4e4eb551b41df0c42372",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9666aa483a8c404ab6dcc0c57ad085b8",
      "value": 29
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
